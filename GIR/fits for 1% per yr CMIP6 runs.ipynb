{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from statsmodels.api import OLS\n",
    "import statsmodels.tools.tools\n",
    "from pandas import DataFrame\n",
    "\n",
    "# # import sys to add path to datasets folder\n",
    "# import sys\n",
    "# sys.path.append('/Users/stuartjenkins/Documents/$$Datasets/GMST')\n",
    "\n",
    "from GIR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fair_scm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9fcbdc5f3d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfair_scm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fair_scm'"
     ]
    }
   ],
   "source": [
    "## Python code to import and process the different historical temperature observation datasets used in Chapter 1, SR1.5. \n",
    "\n",
    "# Written by Stuart Jenkins (stuart.jenkins@wadham.ox.ac.uk) (18/12/2018)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Import and rebaseline the observations ready for plotting\n",
    "# -------------------------------------------------\n",
    "def temp_import():\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports the HadCRUT4, HadCRUT4-CW, NOAA and GISTEMP datasets, re-baselines them to 1850-1900\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the baseline year range, and common reference range\n",
    "    base_low=1850.\n",
    "    base_high=1900.\n",
    "    com_ref_low=1880.\n",
    "    com_ref_high=2017.\n",
    "    # define variable representing the frequency of temperature observations data ('mon' = monthly)\n",
    "    temp_freq='mon'\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    ## Import the temperature observation datasets ##\n",
    "    #Specify the GMST best-estimate temperature timeseries files to load from\n",
    "    gmst_files = {'HadCRUT4':'../../../$$Datasets/GMST/HadCRUT.4.6.0.0.monthly_ns_avg.txt',\n",
    "    'GISTEMP':'../../../$$Datasets/GMST/GLB.Ts+dSST.csv',\n",
    "    'NOAA':'../../../$$Datasets/GMST/aravg.mon.land_ocean.90S.90N.v4.0.1.201803.asc',\n",
    "    'Cowtan-Way':'../../../$$Datasets/GMST/had4_krig_v2_0_0.txt'}\n",
    "\n",
    "    gmst_names = gmst_files.keys()\n",
    "    # make a common years vector, which we can use as the years variable on all imported temperature datasets\n",
    "    years_com = np.arange(1850. + 1./24,1850. + 1./24 + (2020)*1./12,1.0/12)[:-1]\n",
    "\n",
    "    # define dictionary gmst to hold the temperature data and its averages etc.\n",
    "    gmst = {}\n",
    "\n",
    "    # Go through the datasets imported from the files referenced in 'gmst_files' above and load them\n",
    "    for key in gmst_names:\n",
    "\n",
    "        if key in ['HadCRUT4','Cowtan-Way']:\n",
    "            data = np.genfromtxt(gmst_files[key])\n",
    "            temps = data[:,1]\n",
    "            years = years_com[:len(temps)]\n",
    "\n",
    "        if key in ['GISTEMP']:\n",
    "            f_giss = open(gmst_files[key],'r')\n",
    "            temps = []\n",
    "            counter = 0\n",
    "            for line in f_giss:\n",
    "              if counter>=2:\n",
    "                  temps.extend([float(f) for f in line.split(',')[1:13] if f != '***'])\n",
    "              counter = counter + 1\n",
    "            temps=np.array(temps)\n",
    "            years = years_com[years_com>1880.][:len(temps)]\n",
    "\n",
    "        if key in ['NOAA']:\n",
    "            data = np.genfromtxt(gmst_files[key])\n",
    "            temps = data[:,2]\n",
    "            years = years_com[years_com>1880.][:len(temps)]\n",
    "\n",
    "\n",
    "        gmst[key] = {'Temp':temps,'Years':years}\n",
    "\n",
    "    #Set the datasets to a common reference period        \n",
    "    hc_ref = np.mean(gmst['HadCRUT4']['Temp'][np.logical_and(gmst['HadCRUT4']['Years']>=com_ref_low,\n",
    "                    gmst['HadCRUT4']['Years']<(com_ref_high+1))]) - np.mean(gmst['HadCRUT4']['Temp'][np.logical_and(gmst['HadCRUT4']['Years']>=base_low,\n",
    "                                                    gmst['HadCRUT4']['Years']<(base_high+1))])\n",
    "    for key in gmst_names:\n",
    "        gmst[key]['Temp'] = gmst[key]['Temp'][gmst[key]['Years'] < 2018.]\n",
    "        gmst[key]['Years'] = gmst[key]['Years'][gmst[key]['Years'] < 2018.]\n",
    "        #Express relative to a common base period\n",
    "        gmst[key]['Temp'] = gmst[key]['Temp'] - np.mean(gmst[key]['Temp'][np.logical_and(gmst[key]['Years']>=com_ref_low,\n",
    "                                                                  gmst[key]['Years']<(com_ref_high+1))])\n",
    "        #Set NOAA and GISTEMP datasets relative to HadCRUT4 value over the base period \n",
    "        if key in ['NOAA','GISTEMP']:\n",
    "            gmst[key]['Temp'] = gmst[key]['Temp'] + hc_ref\n",
    "        else: \n",
    "            gmst[key]['Temp'] = gmst[key]['Temp'] - np.mean(gmst[key]['Temp'][np.logical_and(gmst[key]['Years']>=base_low,gmst[key]['Years']<(base_high+1))])\n",
    "\n",
    "    return gmst\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Find the min, mean and max values from the temperature observations\n",
    "# -----------------------------------------------\n",
    "def calc_mean_min_max(gmst):\n",
    "\n",
    "    \"\"\"\n",
    "    Requires gmst to have dictionary strings: HadCRUT4, Cowtan-Way, GISTEMP, NOAA\n",
    "    \"\"\"\n",
    "    \n",
    "    obs_max = np.zeros_like(gmst['HadCRUT4']['Years'])\n",
    "    obs_min = np.zeros_like(gmst['HadCRUT4']['Years'])\n",
    "    obs_mean = np.zeros_like(gmst['HadCRUT4']['Years'])\n",
    "    for y in range(0,len(gmst['HadCRUT4']['Years'])): \n",
    "        year_vals = []\n",
    "        #Loop over AR5 datasets and Cowtan-Way\n",
    "        for ob in ['HadCRUT4','NOAA','GISTEMP','Cowtan-Way']:\n",
    "            # collect the temperature value at a given year in each dataset and store in val\n",
    "            val = gmst[ob]['Temp'][gmst[ob]['Years']==gmst['HadCRUT4']['Years'][y]]\n",
    "            if len(val)>0:\n",
    "                year_vals.append(val)\n",
    "        # find the min, mean and max values from each year\n",
    "        obs_max[y] = np.max(year_vals)\n",
    "        obs_min[y] = np.min(year_vals)\n",
    "        obs_mean[y] = np.mean(year_vals)\n",
    "\n",
    "    # save as entries in gmst\n",
    "    gmst['Temp-max'] = obs_max\n",
    "    gmst['Temp-min'] = obs_min\n",
    "    gmst['Temp-mean'] = obs_mean\n",
    "    \n",
    "    return gmst\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Using OLS regression to scale anthropogenic and natural contributions to observed GMST data\n",
    "# Methodology follows Haustein et al. (Scientific Reports, 2017)\n",
    "# -----------------------------------------------\n",
    "def calc_gwi(obs,obs_years,reg_type='mon',base_low=1850.,base_high=1900, name=''):\n",
    "    \n",
    "    #Express the observations relative to the base period \n",
    "    obs = obs - np.mean(obs[np.logical_and(obs_years>=base_low,obs_years<(base_high+1))])\n",
    "\n",
    "    #Load the best estimate forcings from Piers\n",
    "    forc_file = '../../../$$Datasets/RF/AWI_all_forcing_CH4updated.txt'\n",
    "    data = np.genfromtxt(forc_file,skip_header=1)\n",
    "    years = data[:,0]\n",
    "    tot_forc = data[:,2]\n",
    "    ant_forc = data[:,3]\n",
    "    \n",
    "#     #Integrate anthropogenic and natural forcing with standard FAIR parameters\n",
    "#     C, t_nat = fair_scm(other_rf=tot_forc-ant_forc)\n",
    "#     C, t_anthro = fair_scm(other_rf=ant_forc)\n",
    "#     #Express relative to the centre of the base period\n",
    "#     t_nat = t_nat - np.mean(t_nat[np.logical_and(years>=base_low,years<base_high+1)])\n",
    "#     t_anthro = t_anthro - np.mean(t_anthro[np.logical_and(years>=base_low,years<base_high+1)])\n",
    "#     # -----------------------------------------------\n",
    "    \n",
    "    \n",
    "#     # Prepare the temperatures run through FaIR, so they lie on same year-grid as observations, so they can be compared\n",
    "#     # -----------------------------------------------\n",
    "#     #Interpolate the annual forced responses to the grid of the observed data\n",
    "#     if reg_type !='mon':\n",
    "#         t_nat = np.interp(obs_years+0.5, years+0.5, t_nat)\n",
    "#         t_anthro = np.interp(obs_years+0.5, years+0.5, t_anthro)\n",
    "#     else:\n",
    "#         t_nat = np.interp(obs_years, years+0.5, t_nat)\n",
    "#         t_anthro = np.interp(obs_years, years+0.5, t_anthro)\n",
    "\n",
    "#     #Linearly project the final half year\n",
    "#     t_anthro[obs_years>(years[-1]+0.5)] = 12*(t_anthro[obs_years<=(years[-1]+0.5)][-1] - t_anthro[obs_years<=(years[-1]+0.5)][-2]) * (obs_years[obs_years>(years[-1]+0.5)] - obs_years[obs_years<=(years[-1]+0.5)][-1]) \\\n",
    "#     +t_anthro[obs_years<=(years[-1]+0.5)][-1]\n",
    "#     t_nat[obs_years>(years[-1]+0.5)] = 12*(t_nat[obs_years<=(years[-1]+0.5)][-1] - t_nat[obs_years<=(years[-1]+0.5)][-2]) * (obs_years[obs_years>(years[-1]+0.5)] - obs_years[obs_years<=(years[-1]+0.5)][-1]) \\\n",
    "#     +t_nat[obs_years<=(years[-1]+0.5)][-1]\n",
    "#     # -----------------------------------------------\n",
    "    \n",
    "#     #Use scipy defined OLS regression function to complete OLD regression of observations data on natural and anthropogenic warming with a constant\n",
    "#     y = np.copy(obs)\n",
    "#     x = DataFrame({'x1': (t_anthro), 'x2': (t_nat)})\n",
    "#     # add constant vector on to dataframe we will fit to temp observations\n",
    "#     x = statsmodels.tools.tools.add_constant(x)\n",
    "#     # complete OLS regression of anthropogenic and natural temperatures (found from FaIR integrated best estimate forcing) onto given observed temperature dataset.\n",
    "#     model = OLS(y, x)\n",
    "#     result = model.fit()\n",
    "#     # collect output scaling factors for anthro and natural temperature timeseries\n",
    "#     sf = result.params\n",
    "\n",
    "#     #Form scaled anthropgenic warming index\n",
    "#     awi = t_anthro * sf['x1']\n",
    "#     #Scaled natural warming index\n",
    "#     nwi = t_nat * sf['x2']\n",
    "#     #Scaled total externally forced warming index\n",
    "#     gwi = awi + nwi\n",
    "    \n",
    "#     print(name, ' AWI scale factor: ', sf['x1'], '\\n', name, ' NWI scale factor: ', sf['x2'])\n",
    "\n",
    "#     return awi, nwi\n",
    "    return\n",
    "\n",
    "# -------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method:\n",
    "# Using attributable warming plus GCP emissions\n",
    "# Fit r0 and rT / rC (fixed ratio) to present day concentrations\n",
    "\n",
    "fit_time_period = list(set(Best_emission_estimates['default','CO2'].dropna().index).intersection(set(Attributable_warming.index)))\n",
    "\n",
    "CO2_fit_warming = Attributable_warming.loc[fit_time_period].values.flatten()\n",
    "CO2_fit_emissions = convert_forc_to_model_input(Best_emission_estimates['default','CO2'],'fit_CO2','CO2')\n",
    "\n",
    "CO2_original_parameters = convert_forc_to_model_input(default_gas_forcing_params()['default','CO2'],'tune_CO2','CO2')\n",
    "\n",
    "def fit_CO2_params(x,fit_time):\n",
    "    \n",
    "    fit_params = CO2_original_parameters.copy()\n",
    "    rT_rC_scaling = 0.019/4.165\n",
    "    fit_params.loc[['r0','rT','rC'],('tune_CO2','CO2')] = [ x[0] , x[1] , x[1] * rT_rC_scaling ]\n",
    "    \n",
    "    fit_model_run = prescribed_temps_gas_cycle(T=CO2_fit_warming,emissions_in=CO2_fit_emissions,gas_parameters=fit_params)['C']\n",
    "    \n",
    "    return np.sum((CMIP6_concs_extended.loc[2017,'CO2'] - fit_model_run.loc[2017,('fit_CO2','tune_CO2','CO2')])**2)\n",
    "\n",
    "fit_result = sp.optimize.minimize(fit_CO2_params,x0=[32,4.165],args=2017,method='Nelder-mead')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(CMIP6_concs_extended.loc[fit_time_period,'CO2'],'k',label='CMIP6 historical')\n",
    "tuned_params = CO2_original_parameters.copy()\n",
    "rT_rC_scaling = 0.019/4.165\n",
    "tuned_params.loc[['r0','rT','rC'],('tune_CO2','CO2')] = [ fit_result.x[0] , fit_result.x[1] , fit_result.x[1] * rT_rC_scaling ]\n",
    "tuned_model_run = prescribed_temps_gas_cycle(T=CO2_fit_warming,emissions_in=CO2_fit_emissions,gas_parameters=tuned_params)['C'].loc[fit_time_period]\n",
    "ax.plot(tuned_model_run,'r',label='FaIR v2')\n",
    "plt.xlim(1850,2017)\n",
    "plt.title('Observed vs best-fit modelled CO$_2$ concentrations')\n",
    "plt.legend()\n",
    "\n",
    "print('r0:',fit_result.x[0])\n",
    "print('rT:',fit_result.x[1])\n",
    "print('rC:',fit_result.x[1] * rT_rC_scaling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
